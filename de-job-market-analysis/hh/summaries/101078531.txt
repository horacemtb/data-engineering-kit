Мы ждем от будущих коллег: Уверенное владение Python; Опыт использования эко-системы Hadoop: HDFS, Apache AirFlow, Hive, Kafka,Spark; Знание SQL; Опыт работы с реляционными базами данных (Oracle). Ваши задачи: Реализация ETL в Hadoop (с помощью Airflow); Работа с различными источниками данных: Oracle, MS SQL, API личных кабинетов, микросервисы; Батч и стримы с помощью PySpark и Kafka; Подготовка витрин для анализа (Hive + Spark+ SQL). Наш стек: Ванильный hadoop; Kafka, Spark, Airflow; ClickHouse; Jira; Confluence; GitLab.