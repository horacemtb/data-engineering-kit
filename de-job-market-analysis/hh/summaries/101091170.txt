Задачи: исследовать источники данных (внешних данных, реплик АС Банка, других витрин) разрабатывать и менять витрины данных на Hadoop исходя из требований аналитиков данных создавать алгоритмы загрузки данных в витрины с учётом историчности, уникальности, логики обновления таблиц писать функции для первичной обработки, преобразования и агрегации данных на Spark (Scala, Java, Python) Code Review изменений, предлагаемых коллегами из своей и соседних команд. Наша команда будет заниматься разработкой и выводом витрин данных в промышленный контур (Hadoop), исследованием новых источников данных, строить интеграцию со смежными системами. Ждём от кандидата: знаешь SQL на уровне аналитических запросов и оптимизация запросов умеешь обращаться с git, bash знаешь языки программирования Java/Scala, Python есть понимание работы Hive, Spark на Hadoop, парадигмы MapReduce опыт разработки на Spark/PySpark, оптимизации выполнения сценариев на Spark имеешь опыт использования Apache Oozie понимаешь процессы ETL, ELT.