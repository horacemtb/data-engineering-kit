This includes working with technologies like Hadoop, Oozie, Pig, Hive, Map Reduce, Spark (Java), Python, Hbase - Develop, Manage and optimize data workflows using Oozie and Airflow within the Apache Hadoop ecosystem - Leverage GCP for scalable big data processing and storage solutions - Implementing automation/DevOps best practices for CI/CD, IaC, etc. This includes working with technologies like Hadoop, Oozie, Pig, Hive, Map Reduce, Spark (Java), Python, Hbase - Develop, Manage and optimize data workflows using Oozie and Airflow within the Apache Hadoop ecosystem - Leverage GCP for scalable big data processing and storage solutions - Implementing automation/DevOps best practices for CI/CD, IaC, etc. The ideal candidate will have a strong background in developing batch processing systems, with extensive experience in Oozie, the Apache Hadoop ecosystem, Airflow, and a solid understanding of public cloud technologies, especially GCP.